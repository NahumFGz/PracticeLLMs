{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Redis Cache for LangChain\n",
    "https://python.langchain.com/docs/integrations/caches/redis_llm_caching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"Connecting to Redis at: {REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain_redis import RedisCache\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (not cached):\n",
      "Result: content='Caching is a technique used to store frequently accessed data in a temporary storage area, known as a cache, to improve retrieval speed and reduce latency. By keeping copies of data closer to the user or application, caching minimizes the need to repeatedly fetch the same information from slower storage or remote servers. This results in enhanced performance and efficiency, particularly in web applications and databases, where quick access to data is crucial.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 16, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-84f8884b-c854-4204-a812-3ca74af0b41e-0' usage_metadata={'input_tokens': 16, 'output_tokens': 82, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Time: 2.20 seconds\n",
      "\n",
      "Second call (cached):\n",
      "Result: content='Caching is a technique used to store frequently accessed data in a temporary storage area, known as a cache, to improve retrieval speed and reduce latency. By keeping copies of data closer to the user or application, caching minimizes the need to repeatedly fetch the same information from slower storage or remote servers. This results in enhanced performance and efficiency, particularly in web applications and databases, where quick access to data is crucial.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 16, 'total_tokens': 98, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-84f8884b-c854-4204-a812-3ca74af0b41e-0' usage_metadata={'input_tokens': 16, 'output_tokens': 82, 'total_tokens': 98, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Time: 0.00 seconds\n",
      "\n",
      "Speed improvement: 584.40x faster\n",
      "Cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\" ,temperature=0)\n",
    "\n",
    "# Function to measure execution time\n",
    "def timed_completion(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# First call (not cached)\n",
    "prompt = \"Explain the concept of caching in three sentences.\"\n",
    "result1, time1 = timed_completion(prompt)\n",
    "print(f\"First call (not cached):\\nResult: {result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Second call (should be cached)\n",
    "result2, time2 = timed_completion(prompt)\n",
    "print(f\"Second call (cached):\\nResult: {result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the cache\n",
    "redis_cache.clear()\n",
    "print(\"Cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"key\":\"value\"}'\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "r = redis.Redis()\n",
    "r.execute_command('JSON.SET', 'test_key', '.', '{\"key\": \"value\"}')\n",
    "print(r.execute_command('JSON.GET', 'test_key'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_langchain_px",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
