{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Conversational RAG using Memory](https://haystack.deepset.ai/cookbook/conversational_rag_using_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Prueba de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from datasets import load_dataset\n",
    "\n",
    "#! Preparar documentos\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
    "\n",
    "#! Escribir documentos\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.chat_message_stores.in_memory import InMemoryChatMessageStore\n",
    "from haystack_experimental.components.retrievers import ChatMessageRetriever\n",
    "from haystack_experimental.components.writers import ChatMessageWriter\n",
    "\n",
    "#! Memory components\n",
    "memory_store = InMemoryChatMessageStore()\n",
    "memory_retriever = ChatMessageRetriever(memory_store)\n",
    "memory_writer = ChatMessageWriter(memory_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "#! Prompt Template for RAG with Memory\n",
    "system_message = ChatMessage.from_system(\"You are a helpful AI assistant using provided supporting documents and conversation history to assist humans\")\n",
    "\n",
    "user_message_template =\"\"\"\n",
    "    Given the conversation history and the provided supporting documents, give a brief answer to the question.\n",
    "    Note that supporting documents are not part of the conversation. If question can't be answered from supporting documents, say so.\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.text }}\n",
    "    {% endfor %}\n",
    "\n",
    "    Supporting documents:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{query}}\n",
    "    \\nAnswer:\n",
    "\"\"\"\n",
    "user_message = ChatMessage.from_user(user_message_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import Any\n",
    "\n",
    "from haystack import component\n",
    "from haystack.core.component.types import Variadic\n",
    "\n",
    "#! Build the Pipeline\n",
    "\n",
    "@component\n",
    "class ListJoiner:\n",
    "    def __init__(self, _type: Any):\n",
    "        component.set_output_types(self, values=_type)\n",
    "\n",
    "    def run(self, values: Variadic[Any]):\n",
    "        result = list(chain(*values))\n",
    "        return {\"values\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fd04b6afa90>\n",
       "ðŸš… Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: OpenAIChatGenerator\n",
       "  - memory_retriever: ChatMessageRetriever\n",
       "  - memory_writer: ChatMessageWriter\n",
       "  - memory_joiner: ListJoiner\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])\n",
       "  - llm.replies -> memory_joiner.values (List[ChatMessage])\n",
       "  - memory_retriever.messages -> prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_joiner.values -> memory_writer.messages (List[ChatMessage])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.converters import OutputAdapter\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# components for RAG\n",
    "pipeline.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store, top_k=3))\n",
    "pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(variables=[\"query\", \"documents\", \"memories\"], required_variables=[\"query\", \"documents\", \"memories\"]))\n",
    "pipeline.add_component(\"llm\", OpenAIChatGenerator())\n",
    "\n",
    "# components for memory\n",
    "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
    "pipeline.add_component(\"memory_writer\", memory_writer)\n",
    "pipeline.add_component(\"memory_joiner\", ListJoiner(List[ChatMessage]))\n",
    "\n",
    "# connections for RAG\n",
    "pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "pipeline.connect(\"llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for memory\n",
    "pipeline.connect(\"memory_joiner\", \"memory_writer\")\n",
    "pipeline.connect(\"memory_retriever\", \"prompt_builder.memories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– The Rhodes statue, known as the Colossus of Rhodes, is believed to have represented Helios, with a standard rendering of a head featuring curly hair and evenly spaced spikes of bronze or silver flame radiating from it, similar to images found on contemporary Rhodian coins. However, the exact appearance of the statue remains uncertain as there are no surviving depictions.\n",
      "ðŸ¤– The Mausoleum of Halicarnassus was built by Artemisia, the queen of Caria, as a tomb for her husband, Mausolus.\n",
      "ðŸ¤– I'm sorry, but there is no information provided in the conversation history or supporting documents to answer the question \"q.\" Please provide more context or details for assistance.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    messages = [system_message, user_message]\n",
    "    question = input(\"Enter your question or Q to exit.\\nðŸ§‘ \")\n",
    "    if question==\"Q\":\n",
    "        break\n",
    "\n",
    "    res = pipeline.run(data={\"retriever\": {\"query\": question},\n",
    "                             \"prompt_builder\": {\"template\": messages, \"query\": question},\n",
    "                             \"memory_joiner\": {\"values\": [ChatMessage.from_user(question)]}},\n",
    "                            include_outputs_from=[\"llm\"])\n",
    "    assistant_resp = res['llm']['replies'][0]\n",
    "    print(f\"ðŸ¤– {assistant_resp.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Plantilla de solucitud para reformular la consulta del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rephrase_template = \"\"\"\n",
    "        Rewrite the question for search while keeping its meaning and key terms intact.\n",
    "        If the conversation history is empty, DO NOT change the query.\n",
    "        Use conversation history only if necessary, and avoid extending the query with your own knowledge.\n",
    "        If no changes are needed, output the current question as is.\n",
    "\n",
    "        Conversation history:\n",
    "        {% for memory in memories %}\n",
    "            {{ memory.text }}\n",
    "        {% endfor %}\n",
    "\n",
    "        User Query: {{query}}\n",
    "        Rewritten Query:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fd04b6af150>\n",
       "ðŸš… Components\n",
       "  - query_rephrase_prompt_builder: PromptBuilder\n",
       "  - query_rephrase_llm: OpenAIGenerator\n",
       "  - list_to_str_adapter: OutputAdapter\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: OpenAIChatGenerator\n",
       "  - memory_retriever: ChatMessageRetriever\n",
       "  - memory_writer: ChatMessageWriter\n",
       "  - memory_joiner: ListJoiner\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - query_rephrase_prompt_builder.prompt -> query_rephrase_llm.prompt (str)\n",
       "  - query_rephrase_llm.replies -> list_to_str_adapter.replies (List[str])\n",
       "  - list_to_str_adapter.output -> retriever.query (str)\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])\n",
       "  - llm.replies -> memory_joiner.values (List[ChatMessage])\n",
       "  - memory_retriever.messages -> query_rephrase_prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_joiner.values -> memory_writer.messages (List[ChatMessage])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.converters import OutputAdapter\n",
    "\n",
    "conversational_rag = Pipeline()\n",
    "\n",
    "# components for query rephrasing\n",
    "conversational_rag.add_component(\"query_rephrase_prompt_builder\", PromptBuilder(query_rephrase_template))\n",
    "conversational_rag.add_component(\"query_rephrase_llm\", OpenAIGenerator())\n",
    "conversational_rag.add_component(\"list_to_str_adapter\", OutputAdapter(template=\"{{ replies[0] }}\", output_type=str))\n",
    "\n",
    "# components for RAG\n",
    "conversational_rag.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store, top_k=3))\n",
    "conversational_rag.add_component(\"prompt_builder\", ChatPromptBuilder(variables=[\"query\", \"documents\", \"memories\"], required_variables=[\"query\", \"documents\", \"memories\"]))\n",
    "conversational_rag.add_component(\"llm\", OpenAIChatGenerator())\n",
    "\n",
    "# components for memory\n",
    "conversational_rag.add_component(\"memory_retriever\", ChatMessageRetriever(memory_store))\n",
    "conversational_rag.add_component(\"memory_writer\", ChatMessageWriter(memory_store))\n",
    "conversational_rag.add_component(\"memory_joiner\", ListJoiner(List[ChatMessage]))\n",
    "\n",
    "# connections for query rephrasing\n",
    "conversational_rag.connect(\"memory_retriever\", \"query_rephrase_prompt_builder.memories\")\n",
    "conversational_rag.connect(\"query_rephrase_prompt_builder.prompt\", \"query_rephrase_llm\")\n",
    "conversational_rag.connect(\"query_rephrase_llm.replies\", \"list_to_str_adapter\")\n",
    "conversational_rag.connect(\"list_to_str_adapter\", \"retriever.query\")\n",
    "\n",
    "# connections for RAG\n",
    "conversational_rag.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "conversational_rag.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "conversational_rag.connect(\"llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for memory\n",
    "conversational_rag.connect(\"memory_joiner\", \"memory_writer\")\n",
    "conversational_rag.connect(\"memory_retriever\", \"prompt_builder.memories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ”Ž Search Query: Where can I find the Gardens of Babylon?\n",
      "ðŸ¤– The exact location of the Hanging Gardens of Babylon is uncertain, as no archaeological evidence has been definitively found in Babylon itself. Some theories suggest they might have been a mythical creation, or that they could refer to the gardens constructed by the Assyrian king Sennacherib in Nineveh, not Babylon.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    messages = [system_message, user_message]\n",
    "    question = input(\"Enter your question or Q to exit.\\nðŸ§‘ \")\n",
    "    if question==\"Q\":\n",
    "        break\n",
    "\n",
    "    res = conversational_rag.run(data={\"query_rephrase_prompt_builder\": {\"query\": question},\n",
    "                             \"prompt_builder\": {\"template\": messages, \"query\": question},\n",
    "                             \"memory_joiner\": {\"values\": [ChatMessage.from_user(question)]}},\n",
    "                            include_outputs_from=[\"llm\",\"query_rephrase_llm\"])\n",
    "    search_query = res['query_rephrase_llm']['replies'][0]\n",
    "    print(f\"   ðŸ”Ž Search Query: {search_query}\")\n",
    "    assistant_resp = res['llm']['replies'][0]\n",
    "    print(f\"ðŸ¤– {assistant_resp.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasos para integrar con django\n",
    "#! https://chatgpt.com/c/67635552-1d10-8010-96c6-5094597525a8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_haystack_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
